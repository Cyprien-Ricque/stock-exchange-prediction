{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:matplotlib.pyplot:Loaded backend module://matplotlib_inline.backend_inline version unknown.\n"
     ]
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ed61c0b170ba42dfbc22a0f8dc7c3f57"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": "42"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from utilities import *\n",
    "from data_factory.preprocessing import *\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm, notebook\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")\n",
    "notebook.tqdm().pandas()\n",
    "\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'device': 'cpu',\n 'seed': False,\n 'data': {'save': '../data/save/',\n  'train_path': '../data/jpx-tokyo-stock-exchange-prediction/train_files/',\n  'test_path': '../data/jpx-tokyo-stock-exchange-prediction/supplemental_files/',\n  'financials': 'financials.csv',\n  'stock_prices': 'stock_prices.csv',\n  'options': 'options.csv',\n  'secondary_stock_price': 'secondary_stock_price.csv',\n  'trades': 'trades.csv'},\n 'sliding_window': {'max_prediction_length': 10,\n  'min_prediction_length': 5,\n  'max_encoder_length': 50,\n  'min_encoder_length': 50,\n  'batch_size': 64},\n 'model': {'name': 'gmm', 'path': './cache/', 'n_clusters': 4},\n 'optimizer': {'name': 'adam',\n  'epochs': 10,\n  'params': {'lr': 0.001, 'regularization': 0.0001}}}"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = load_config(\"../config/config.yml\")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_path = config['data']['train_path']\n",
    "test_path = config['data']['test_path']\n",
    "stock_prices = config['data']['stock_prices']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Stock prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "           RowId       Date  SecuritiesCode    Open    High     Low   Close  \\\n0  20170104_1301 2017-01-04            1301  2734.0  2755.0  2730.0  2742.0   \n1  20170104_1332 2017-01-04            1332   568.0   576.0   563.0   571.0   \n\n    Volume  AdjustmentFactor  ExpectedDividend  SupervisionFlag    Target  \n0    31400               1.0               NaN            False  0.000730  \n1  2798500               1.0               NaN            False  0.012324  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RowId</th>\n      <th>Date</th>\n      <th>SecuritiesCode</th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Volume</th>\n      <th>AdjustmentFactor</th>\n      <th>ExpectedDividend</th>\n      <th>SupervisionFlag</th>\n      <th>Target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20170104_1301</td>\n      <td>2017-01-04</td>\n      <td>1301</td>\n      <td>2734.0</td>\n      <td>2755.0</td>\n      <td>2730.0</td>\n      <td>2742.0</td>\n      <td>31400</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>0.000730</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20170104_1332</td>\n      <td>2017-01-04</td>\n      <td>1332</td>\n      <td>568.0</td>\n      <td>576.0</td>\n      <td>563.0</td>\n      <td>571.0</td>\n      <td>2798500</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>0.012324</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f'{train_path}/{stock_prices}', parse_dates=[\"Date\"])\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "           RowId       Date  SecuritiesCode    Open    High     Low   Close  \\\n0  20211206_1301 2021-12-06            1301  2982.0  2982.0  2965.0  2971.0   \n1  20211206_1332 2021-12-06            1332   592.0   599.0   588.0   589.0   \n\n    Volume  AdjustmentFactor  ExpectedDividend  SupervisionFlag    Target  \n0     8900               1.0               NaN            False -0.003263  \n1  1360800               1.0               NaN            False -0.008993  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RowId</th>\n      <th>Date</th>\n      <th>SecuritiesCode</th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Volume</th>\n      <th>AdjustmentFactor</th>\n      <th>ExpectedDividend</th>\n      <th>SupervisionFlag</th>\n      <th>Target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20211206_1301</td>\n      <td>2021-12-06</td>\n      <td>1301</td>\n      <td>2982.0</td>\n      <td>2982.0</td>\n      <td>2965.0</td>\n      <td>2971.0</td>\n      <td>8900</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>-0.003263</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20211206_1332</td>\n      <td>2021-12-06</td>\n      <td>1332</td>\n      <td>592.0</td>\n      <td>599.0</td>\n      <td>588.0</td>\n      <td>589.0</td>\n      <td>1360800</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>-0.008993</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(f'{test_path}/{stock_prices}', parse_dates=[\"Date\"])\n",
    "df_test.head(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['Timestamp'] = date_to_timestamp['1d'](df.Date.values.astype(np.int64)).astype(int)\n",
    "df_test['Timestamp'] = date_to_timestamp['1d'](df_test.Date.values.astype(np.int64)).astype(int)\n",
    "\n",
    "df.SupervisionFlag = df.SupervisionFlag.astype('category')\n",
    "df.SecuritiesCode = df.SecuritiesCode.astype(str)\n",
    "\n",
    "df_test.SupervisionFlag = df_test.SupervisionFlag.astype('category')\n",
    "df_test.SecuritiesCode = df_test.SecuritiesCode.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fill na"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "RowId                     0\nDate                      0\nSecuritiesCode            0\nOpen                   7608\nHigh                   7608\nLow                    7608\nClose                  7608\nVolume                    0\nAdjustmentFactor          0\nExpectedDividend    2313666\nSupervisionFlag           0\nTarget                  238\nTimestamp                 0\ndtype: int64"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "RowId                    0\nDate                     0\nSecuritiesCode           0\nOpen                   284\nHigh                   284\nLow                    284\nClose                  284\nVolume                   0\nAdjustmentFactor         0\nExpectedDividend    111497\nSupervisionFlag          0\nTarget                   0\nTimestamp                0\ndtype: int64"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.isna().sum(axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing targets train 238\n",
      "Missing targets test 0\n"
     ]
    }
   ],
   "source": [
    "print(f'Missing targets train {df.Target.isna().sum()}')\n",
    "df.dropna(subset=['Target'], inplace=True)\n",
    "print(f'Missing targets test {df_test.Target.isna().sum()}')\n",
    "df_test.dropna(subset=['Target'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.ExpectedDividend.fillna(value=0, inplace=True)\n",
    "df.loc[:, ['Open', 'High', 'Low', 'Close']] = df.loc[:, ['Open', 'High', 'Low', 'Close']].fillna(method='ffill')\n",
    "df_test.ExpectedDividend.fillna(value=0, inplace=True)\n",
    "df_test.loc[:, ['Open', 'High', 'Low', 'Close']] = df_test.loc[:, ['Open', 'High', 'Low', 'Close']].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum(axis=0).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.isna().sum(axis=0).any()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_train = df.copy()\n",
    "df_train.sort_values(by=['SecuritiesCode', 'Timestamp'], inplace=True)\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_test.sort_values(by=['SecuritiesCode', 'Timestamp'], inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_prediction_length = 10\n",
    "min_prediction_length = 2  # For testing\n",
    "max_encoder_length = 30\n",
    "\n",
    "is_training = df_train.groupby('SecuritiesCode').apply(lambda x: x.Timestamp < (x.Timestamp.max() - max_prediction_length)).reset_index(drop=True)\n",
    "\n",
    "training = TimeSeriesDataSet(df_train[is_training], time_idx='Timestamp', target='Close', group_ids=['SecuritiesCode'],\n",
    "                             allow_missing_timesteps=True,\n",
    "                             static_categoricals=['SecuritiesCode'],\n",
    "                             time_varying_unknown_reals=['Open', 'High', 'Low', 'Close', 'Volume'],\n",
    "                             time_varying_unknown_categoricals=['SupervisionFlag'],\n",
    "                             time_varying_known_reals=['Timestamp'],\n",
    "                             # min_encoder_length=345,\n",
    "                             max_encoder_length=max_encoder_length,\n",
    "                             max_prediction_length=max_prediction_length,\n",
    "                             # scalers={col: DummyScaler() for col in  ['Open', 'High', 'Low', 'Close', 'Volume', 'AdjustmentFactor', 'ExpectedDividend']}\n",
    "                             # target_normalizer=GroupNormalizer(\n",
    "                             #     groups=['SecuritiesCode'], transformation=\"softplus\"\n",
    "                             # ),  # use softplus and normalize by group\n",
    "                             target_normalizer=None\n",
    "                             # add_relative_time_idx=True,\n",
    "                             # add_target_scales=True,\n",
    "                             # add_encoder_length=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "        Timestamp          RowId       Date SecuritiesCode    Open    High  \\\n0           18967  20211206_1301 2021-12-06           1301  2982.0  2982.0   \n1           18968  20211207_1301 2021-12-07           1301  2998.0  3065.0   \n2           18969  20211208_1301 2021-12-08           1301  3080.0  3080.0   \n3           18970  20211209_1301 2021-12-09           1301  3050.0  3085.0   \n4           18971  20211210_1301 2021-12-10           1301  3100.0  3105.0   \n...           ...            ...        ...            ...     ...     ...   \n115995      19047  20220224_9997 2022-02-24           9997   709.0   725.0   \n115996      19048  20220225_9997 2022-02-25           9997   725.0   738.0   \n115997      19051  20220228_9997 2022-02-28           9997   731.0   737.0   \n115998      19052              0 2022-03-01           9997     0.0     0.0   \n115999      19053              0 2022-03-02           9997     0.0     0.0   \n\n           Low   Close    Volume  AdjustmentFactor  ExpectedDividend  \\\n0       2965.0  2971.0    8900.0               1.0               0.0   \n1       2990.0  3065.0   19100.0               1.0               0.0   \n2       3035.0  3055.0   11600.0               1.0               0.0   \n3       3025.0  3085.0   11700.0               1.0               0.0   \n4       3050.0  3105.0   14700.0               1.0               0.0   \n...        ...     ...       ...               ...               ...   \n115995   708.0   719.0  195600.0               1.0               0.0   \n115996   724.0   733.0  170500.0               1.0               0.0   \n115997   726.0   734.0  288100.0               1.0               0.0   \n115998     0.0     0.0       0.0               1.0               0.0   \n115999     0.0     0.0       0.0               1.0               0.0   \n\n       SupervisionFlag    Target  \n0                False -0.003263  \n1                False  0.009820  \n2                False  0.006483  \n3                False -0.006441  \n4                False -0.008104  \n...                ...       ...  \n115995           False  0.001364  \n115996           False -0.001362  \n115997           False -0.030014  \n115998           False  0.000000  \n115999           False  0.000000  \n\n[116000 rows x 13 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Timestamp</th>\n      <th>RowId</th>\n      <th>Date</th>\n      <th>SecuritiesCode</th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Volume</th>\n      <th>AdjustmentFactor</th>\n      <th>ExpectedDividend</th>\n      <th>SupervisionFlag</th>\n      <th>Target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>18967</td>\n      <td>20211206_1301</td>\n      <td>2021-12-06</td>\n      <td>1301</td>\n      <td>2982.0</td>\n      <td>2982.0</td>\n      <td>2965.0</td>\n      <td>2971.0</td>\n      <td>8900.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>False</td>\n      <td>-0.003263</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>18968</td>\n      <td>20211207_1301</td>\n      <td>2021-12-07</td>\n      <td>1301</td>\n      <td>2998.0</td>\n      <td>3065.0</td>\n      <td>2990.0</td>\n      <td>3065.0</td>\n      <td>19100.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>False</td>\n      <td>0.009820</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>18969</td>\n      <td>20211208_1301</td>\n      <td>2021-12-08</td>\n      <td>1301</td>\n      <td>3080.0</td>\n      <td>3080.0</td>\n      <td>3035.0</td>\n      <td>3055.0</td>\n      <td>11600.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>False</td>\n      <td>0.006483</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>18970</td>\n      <td>20211209_1301</td>\n      <td>2021-12-09</td>\n      <td>1301</td>\n      <td>3050.0</td>\n      <td>3085.0</td>\n      <td>3025.0</td>\n      <td>3085.0</td>\n      <td>11700.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>False</td>\n      <td>-0.006441</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>18971</td>\n      <td>20211210_1301</td>\n      <td>2021-12-10</td>\n      <td>1301</td>\n      <td>3100.0</td>\n      <td>3105.0</td>\n      <td>3050.0</td>\n      <td>3105.0</td>\n      <td>14700.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>False</td>\n      <td>-0.008104</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>115995</th>\n      <td>19047</td>\n      <td>20220224_9997</td>\n      <td>2022-02-24</td>\n      <td>9997</td>\n      <td>709.0</td>\n      <td>725.0</td>\n      <td>708.0</td>\n      <td>719.0</td>\n      <td>195600.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>False</td>\n      <td>0.001364</td>\n    </tr>\n    <tr>\n      <th>115996</th>\n      <td>19048</td>\n      <td>20220225_9997</td>\n      <td>2022-02-25</td>\n      <td>9997</td>\n      <td>725.0</td>\n      <td>738.0</td>\n      <td>724.0</td>\n      <td>733.0</td>\n      <td>170500.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>False</td>\n      <td>-0.001362</td>\n    </tr>\n    <tr>\n      <th>115997</th>\n      <td>19051</td>\n      <td>20220228_9997</td>\n      <td>2022-02-28</td>\n      <td>9997</td>\n      <td>731.0</td>\n      <td>737.0</td>\n      <td>726.0</td>\n      <td>734.0</td>\n      <td>288100.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>False</td>\n      <td>-0.030014</td>\n    </tr>\n    <tr>\n      <th>115998</th>\n      <td>19052</td>\n      <td>0</td>\n      <td>2022-03-01</td>\n      <td>9997</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>False</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>115999</th>\n      <td>19053</td>\n      <td>0</td>\n      <td>2022-03-02</td>\n      <td>9997</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>False</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>116000 rows Ã— 13 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have to add min_prediction_length days so that you have a prediction for the last day of you prediction set.\n",
    "\n",
    "def add_2_days(x: pd.DataFrame):\n",
    "    a = x.loc[:, 'SecuritiesCode'].iloc[0]\n",
    "    x = x.copy().set_index('Timestamp', drop=True)\n",
    "    x = x.reindex(x.index.to_list() + [x.index.max() + 1, x.index.max() + 2])\n",
    "    x.reset_index(drop=False, inplace=True)\n",
    "    x.loc[:, 'SecuritiesCode'] = a\n",
    "    x.loc[:, 'AdjustmentFactor'] = 1.\n",
    "    x.loc[:, 'Date'] = pd.to_datetime(x.Timestamp, unit='d')\n",
    "    x.fillna(0, inplace=True)\n",
    "    return x\n",
    "\n",
    "df_test_ext = df_test.groupby('SecuritiesCode').apply(add_2_days).reset_index(drop=True)\n",
    "df_test_ext.head(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "class TestDataLoader:\n",
    "    \"\"\"\n",
    "    This dataloader is a trick not to have the same label to predict twice. This happens because of\n",
    "    how TimeSeriesDataSet works.\n",
    "    The batch_size MUST be df_test.Timestamp.unique().size which the number of actual labels you want\n",
    "    to predict for each category\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset: TimeSeriesDataSet, batch_size, num_workers=12):\n",
    "        self.data_loader = testing.to_dataloader(train=False, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "    def __iter__(self):  # Trick not to load some values twice\n",
    "        last_group = -1\n",
    "\n",
    "        for X, (y, w) in self.data_loader:\n",
    "            if X['groups'][0] <= last_group:\n",
    "                break\n",
    "            last_group = X['groups'][0]\n",
    "            yield data\n",
    "\n",
    "\n",
    "class TestDataLoader:\n",
    "    \"\"\"\n",
    "    This dataloader is a trick not to have the same label to predict twice. This happens because of\n",
    "    how TimeSeriesDataSet works.\n",
    "    The batch_size MUST be df_test.Timestamp.unique().size which the number of actual labels you want\n",
    "    to predict for each category\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset: TimeSeriesDataSet, batch_size, num_workers=12):\n",
    "        self.data_loader = testing.to_dataloader(train=False, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "    def __iter__(self):  # Trick not to load some values twice\n",
    "        last_group = -1\n",
    "\n",
    "        for X, (y, w) in self.data_loader:\n",
    "            if X['groups'][0] <= last_group:\n",
    "                break\n",
    "            last_group = X['groups'][0]\n",
    "            yield data\n",
    "\n",
    "\n",
    "# create validation set (predict=True) which means to predict the last max_prediction_length points in time\n",
    "# for each series\n",
    "df_train_test = pd.concat([df_train, df_test_ext]).sort_values(by=['SecuritiesCode', 'Timestamp']).reset_index(drop=True)\n",
    "\n",
    "testing = TimeSeriesDataSet.from_dataset(training, df_train_test, predict=False, stop_randomization=True, min_prediction_idx=df_test.Timestamp.min(), min_prediction_length=min_prediction_length)\n",
    "validation = TimeSeriesDataSet.from_dataset(training, df_train, predict=True, stop_randomization=True)\n",
    "\n",
    "# create dataloaders for model\n",
    "batch_size = 128  # set this between 32 to 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=12)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=12)\n",
    "test_dataloader = TestDataLoader(testing, batch_size=df_test.Timestamp.unique().size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object.__new__() takes exactly one argument (the type to instantiate)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [30]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtesting\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.8/typing.py:875\u001B[0m, in \u001B[0;36mGeneric.__new__\u001B[0;34m(cls, *args, **kwds)\u001B[0m\n\u001B[1;32m    873\u001B[0m     obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__new__\u001B[39m(\u001B[38;5;28mcls\u001B[39m)\n\u001B[1;32m    874\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 875\u001B[0m     obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__new__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    876\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m obj\n",
      "\u001B[0;31mTypeError\u001B[0m: object.__new__() takes exactly one argument (the type to instantiate)"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.35 s, sys: 2.75 s, total: 12.1 s\n",
      "Wall time: 23.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for data in test_dataloader:\n",
    "    pass\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataLoader' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [26]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m dl \u001B[38;5;241m=\u001B[39m DataLoader(\u001B[43mtest_dataloader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy\u001B[49m())\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'DataLoader' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(test_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m<timed exec>:1\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    528\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    529\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()\n\u001B[0;32m--> 530\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    531\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    532\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    533\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    534\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py:570\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    568\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    569\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 570\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    571\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    572\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data)\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch\u001B[39m(\u001B[38;5;28mself\u001B[39m, possibly_batched_index):\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[0;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch\u001B[39m(\u001B[38;5;28mself\u001B[39m, possibly_batched_index):\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[0;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "\u001B[0;31mTypeError\u001B[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for data in dl:\n",
    "    pass\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_timestamps = pd.Series(np.arange(df_test.Timestamp.min(), df_test.Timestamp.max() - 2))\n",
    "valid = all_timestamps.isin(df_test.Timestamp.unique())\n",
    "last_group = -1\n",
    "\n",
    "for i, (X, (y, _)) in enumerate(test_dataloader):\n",
    "    print(X['groups'][0])\n",
    "    if X['groups'][0] <= last_group:\n",
    "        break\n",
    "    last_group = X['groups'][0]\n",
    "    print(X['groups'][0])\n",
    "    print(y.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(df_test.Timestamp.max() -2 - df_test.Timestamp.min()) * df_train_test.SecuritiesCode.unique().size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(df_test.Timestamp.max() -2 - df_test.Timestamp.min())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test.Timestamp.unique().size\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for y in a:\n",
    "    print(y[:, :2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Baseline Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_train_test[df_train_test.SecuritiesCode.isin(df_train_test.SecuritiesCode.unique()[:2])].SecuritiesCode.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test_ext.Timestamp.unique().size - 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "e = df_train_test[df_train_test.Timestamp >= df_test.Timestamp.min()]\n",
    "(e.Date.unique().size - min_prediction_length) * e.SecuritiesCode.unique().size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test.Timestamp.unique().size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_dataloader = testing.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a = [y for x, (y, weight) in tqdm(iter(test_dataloader))]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in a:\n",
    "    print(i.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a_ = [y for x, (y, weight) in tqdm(iter(testing))]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in a_:\n",
    "    print(i.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in a_:\n",
    "    print(i.numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f = df_train_test.SecuritiesCode.isin(df_train_test.SecuritiesCode.unique()[:1])\n",
    "f &= (df_train_test.Timestamp >= df_test.Timestamp.min() - max_encoder_length)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test.Timestamp.min()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_train_test.loc[f, ['Close', 'Timestamp']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test_ext.Timestamp.max() - df_test_ext.Timestamp.min()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "min_prediction_length"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(a_))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sum([i.shape[0] for i in a])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a[0].shape, a[1].shape, a[-1].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(a)*1280"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(df_test.Date.unique().size) * df_test.SecuritiesCode.unique().size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "actuals = torch.cat([y for x, (y, weight) in tqdm(iter(test_dataloader))])\n",
    "baseline_predictions = Baseline().predict(test_dataloader)\n",
    "(actuals - baseline_predictions).abs().mean().item()\n",
    "\n",
    "baseline_predictions_np = baseline_predictions.cpu().detach().numpy()\n",
    "actuals_np = actuals.cpu().detach().numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "baseline_predictions.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "actuals.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test.SecuritiesCode.unique().size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_prediction_length, max_encoder_length"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ts_max = df_test.Timestamp.max()\n",
    "s = ts_max - max_prediction_length\n",
    "base = np.arange(s+1, s+max_prediction_length+1)\n",
    "real = df_test[(df_test.Timestamp > ts_max - max_prediction_length) & (df_test.SecuritiesCode == '1301')]['Timestamp'][-max_prediction_length:]\n",
    "f = pd.Series(base).isin(real)\n",
    "\n",
    "df_test_res_baseline = df_test.copy()\n",
    "\n",
    "df_test_res_baseline['close_pred'] = np.nan\n",
    "df_test_res_baseline['close_true'] = np.nan\n",
    "df_test_res_baseline['target_t0_t-1_pred'] = np.nan\n",
    "df_test_res_baseline['target_t0_t-1_true'] = np.nan"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "time_filter = df_test_res_baseline.Timestamp > ts_max - max_prediction_length\n",
    "\n",
    "df_test_res_baseline.loc[(time_filter), 'close_pred'] = baseline_predictions_np[:, f].astype(np.float64).flatten()\n",
    "df_test_res_baseline.loc[(time_filter), 'close_true'] = actuals_np[:, f].astype(np.float64).flatten()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(df_test_res_baseline[df_test_res_baseline.close_true.notna()].Close == df_test_res_baseline[df_test_res_baseline.close_true.notna()].close_true).all()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(df_test_res_baseline.groupby('SecuritiesCode').close_true.diff().astype(str) == df_test_res_baseline.groupby('SecuritiesCode').close_true.diff().reset_index(drop=True).astype(str)).all()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test_res_baseline.groupby('SecuritiesCode').close_true.diff().reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test_res_baseline['target_t0_t-1_pred'] = df_test_res_baseline.groupby('SecuritiesCode').close_pred.diff().reset_index(drop=True)\n",
    "df_test_res_baseline['target_t0_t-1_true'] = df_test_res_baseline.groupby('SecuritiesCode').close_true.diff().reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test_res_baseline['target_t2_t1_pred'] = df_test_res_baseline['target_t0_t-1_pred'].shift(-2)\n",
    "df_test_res_baseline['target_t2_t1_true'] = df_test_res_baseline['target_t0_t-1_true'].shift(-2)\n",
    "\n",
    "df_test_res_baseline['target_pred'] = df_test_res_baseline['target_t2_t1_pred'] / df_test_res_baseline.close_pred.shift(-1)\n",
    "df_test_res_baseline['target_true'] = df_test_res_baseline['target_t2_t1_true'] / df_test_res_baseline.close_true.shift(-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_encoder_length"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test_res_baseline.loc[:, ['target_t0_t-1_pred', 'target_t0_t-1_true', 'target_t2_t1_pred', 'target_t2_t1_true', 'Target', 'target_pred', 'target_true']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Keep only result which have been predicted\n",
    "df_test_res = df_test_res[df_test_res.Timestamp > ts_max - max_prediction_length].copy()\n",
    "df_test_res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test_res['Rank'] = (df_test_res.groupby(\"Date\")[\"Target\"].rank(ascending=False, method=\"first\") - 1).astype(int)\n",
    "df_test_res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utilities.evaluation import calc_spread_return_sharpe\n",
    "\n",
    "calc_spread_return_sharpe(df_test_res)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Temporal Fusion Transformer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# configure network and trainer\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    # clipping gradients is a hyperparameter and important to prevent divergence\n",
    "    # of the gradient for recurrent neural networks\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    # not meaningful for finding the learning rate but otherwise very important\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,  # most important hyperparameter apart from learning rate\n",
    "    # number of attention heads. Set to up to 4 for large datasets\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,  # between 0.1 and 0.3 a!re good values\n",
    "    hidden_continuous_size=8,  # set to <= hidden_size\n",
    "    output_size=7,  # 7 quantiles by default\n",
    "    loss=QuantileLoss(),\n",
    "    # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    reduce_on_plateau_patience=4,\n",
    "    log_interval=-1\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# find optimal learning rate\n",
    "res = trainer.tuner.lr_find(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    "    max_lr=10.0,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# configure network and trainer\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\", \n",
    "    max_epochs=1000,\n",
    "    weights_summary=\"top\",\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,  # coment in for training, running valiation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=3.235936569296285/3,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=7,  # 7 quantiles by default\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# fit network\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target\n",
    "1 2 3 -> 4 5 6 -> Ranking\n",
    "\n",
    "1- define ranking target\n",
    "2- rank"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "actuals = torch.cat([y for x, (y, weight) in iter(test_dataloader)])\n",
    "predictions = tft.predict(test_dataloader)\n",
    "print((actuals - predictions).abs().mean().item())\n",
    "\n",
    "predictions_np = predictions.cpu().detach().numpy()\n",
    "actuals_np = actuals.cpu().detach().numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(actuals - baseline_predictions).abs().mean().item(), (actuals - predictions).abs().mean().item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "baseline_predictions_np[0][0], baseline_predictions[0][0].item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "baseline_predictions_np.shape, baseline_predictions.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions_np[0][0], predictions[0][0].item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ts_max = df_test.Timestamp.max()\n",
    "s = ts_max - max_prediction_length\n",
    "base = np.arange(s+1, s+max_prediction_length+1)\n",
    "real = df_test[(df_test.Timestamp > ts_max - max_prediction_length) & (df_test.SecuritiesCode == '1301')]['Timestamp'][-max_prediction_length:]\n",
    "f = pd.Series(base).isin(real)\n",
    "\n",
    "df_test_res = df_test.copy()\n",
    "df_test_res_baseline = df_test.copy()\n",
    "\n",
    "df_test_res['target_t0_t-1_pred'] = np.nan\n",
    "df_test_res['target_t0_t-1_true'] = np.nan\n",
    "\n",
    "df_test_res_baseline['target_t0_t-1_pred'] = np.nan\n",
    "df_test_res_baseline['target_t0_t-1_true'] = np.nan\n",
    "\n",
    "df_test_res['Target_pred'] = np.nan\n",
    "df_test_res['Target_true'] = np.nan\n",
    "\n",
    "df_test_res_baseline['Target_pred'] = np.nan\n",
    "df_test_res_baseline['Target_true'] = np.nan\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test_res.loc[(df_test_res.Timestamp > ts_max - max_prediction_length), 'target_t0_t-1_pred'] = predictions_np[:, f].flatten()\n",
    "df_test_res.loc[(df_test_res.Timestamp > ts_max - max_prediction_length), 'target_t0_t-1_true'] = actuals_np[:, f].flatten()\n",
    "\n",
    "# df_test_res['Target_true'] = df_test_res['target_t0_t-1_true'].shift(-2)\n",
    "df_test_res = shift(df_test_res, from_='target_t0_t-1_true', to_='target_t0_t-1', shift=2)\n",
    "\n",
    "\n",
    "df_test_res_baseline.loc[(df_test_res.Timestamp > ts_max - max_prediction_length), 'target_t0_t-1_pred'] = baseline_predictions_np[:, f].flatten()\n",
    "df_test_res_baseline.loc[(df_test_res.Timestamp > ts_max - max_prediction_length), 'target_t0_t-1_true'] = actuals_np[:, f].flatten()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(df_test_res.Timestamp > ts_max - max_prediction_length).sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Keep only result which have been predicted\n",
    "df_test_res = df_test_res[df_test_res.Timestamp > ts_max - max_prediction_length].copy()\n",
    "df_test_res_baseline = df_test_res_baseline[df_test_res_baseline.Timestamp > ts_max - max_prediction_length].copy()\n",
    "df_test_res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test_res['Rank'] = (df_test_res.groupby(\"Date\")[\"target_t0_t-1_true\"].rank(ascending=False, method=\"first\") - 1).astype(int)\n",
    "df_test_res_baseline['Rank'] = (df_test_res_baseline.groupby(\"Date\")[\"target_t0_t-1_true\"].rank(ascending=False, method=\"first\") - 1).astype(int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utilities.evaluation import calc_spread_return_sharpe\n",
    "\n",
    "calc_spread_return_sharpe(df_test_res), calc_spread_return_sharpe(df_test_res_baseline)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}